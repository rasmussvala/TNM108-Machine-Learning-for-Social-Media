Part 1: tf-idf (term frequency – inverse document frequency)

    How does the tf-idf work? 

        * tf-idf is a text feature extraction method
        * tf
            * we save all the words from a bunch of documents (d) into vectors 
                * we don't add stop words to the vector (this, is, at, etc) 
                * we only save the frequency of the words (tf)
                * every dimension is a tf for a specific word/term
            * after all vectors are created from every document a matrix of all the vectors is created (often sparse)
        * idf
            * the main problem with the tf approach is that it scales up frequent terms and scales 
              down rare terms which can still be important. 
            * if a word like algebra is used onece in a document it is probably a math document. But the low freqency will remove
              the effect of the word.
            * how to calc the idf: idf =  log ( |Z|/(1 + |{d: t of d}| ) )
              where |Z| is the total number of documents and |{d: t of d}| is the number of documents where the term t occurs.
            * 
        * tf-idf = tf * idf

    Document similarity

        * the similarity of documents can be measured by using the cosine similarity formula 

        * suppose we have a document with the word “sky” appearing 200 times and another document with
          the word “sky” appearing 50, the Euclidean distance between them will be high but the angle will
          still be small because they are pointing to the same direction. This is what matters when we are
          comparing documents

        * the cosine similarity values for different documents, 1 (same direction), 0 (90 deg.), -1 (opposite directions)

    Classifying text

        * One ML method is for classifying text is multinomial naïve Bayes. 
        * What we have to do to use the multinomial naïve Bayes method is to first create a tf-idf of the documents
          then we can create our predicitive model by passing the tf-idf to the multinomial naive Bayes classifier

Part 2: Sentiment analysis

  Show code 

Part 3:  Text summarization

  Explain the TextRank algorithm and how it works 

  * TextRank is a graph-based algorithm
  * TextRank devides the document into sentences
  * Sentences are nodes and edges connect the nodes 
  * The most important sentences are the most connected ones in the graph
  * To identify relations: overlapping words, cosine distance
  * See picture of nodes 

	* TextRank determines the relation of similarity between two sentences based on the content that both
	  share. This overlap is calculated simply as the number of common lexical tokens between them,
	  divided by the length of each to avoid promoting long sentences. 

	* TextRank works by treating a piece of text as a graph, with words or sentences as nodes. It assigns 
	  weights to these nodes based on factors like frequency and similarity. The algorithm then iteratively 
	  adjusts these weights by considering the relationships (edges) between nodes until a stable ranking is achieved. 
	  The final ranked nodes can be used for tasks like text summarization or keyword extraction. 

	Show your lab assistant some summaries you created; and discuss the quality of the summaries

	* See code