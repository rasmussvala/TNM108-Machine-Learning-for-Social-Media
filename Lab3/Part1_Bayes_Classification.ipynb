{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to C:\\Users\\Wille\\scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "# Gauss-Naive Bayes\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "from math import pi\n",
    "from math import e\n",
    "\n",
    "class GaussNB:\n",
    "    summaries={}\n",
    "    target_values=[]\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def group_by_class(self, data, target):\n",
    "        \"\"\"\n",
    "        :param data: Training set\n",
    "        :param target: the list of class labels labelling data\n",
    "        :return:\n",
    "        Separate the data by their target class; that is, create one group for every value of the target class. It returns all the groups        \n",
    "        \"\"\"\n",
    "        separated = [[x for x, t in zip(data, target) if t == c] for c in self.target_values]\n",
    "        groups=[np.array(separated[0]),np.array(separated[1]),np.array(separated[2])]\n",
    "        return np.array(groups)\n",
    "\n",
    "    def summarize(self,data):\n",
    "        \"\"\"\n",
    "        :param data: a dataset whose rows are arrays of features\n",
    "        :return:\n",
    "        the mean and the stdev for each feature of data.\n",
    "        \"\"\"\n",
    "        for index in range(data.shape[1]):\n",
    "            feature_column=data.T[index]\n",
    "            yield{'stdev': statistics.stdev(feature_column),'mean': statistics.mean(feature_column)}\n",
    "\n",
    "    def train(self, data, target):\n",
    "        \"\"\"\n",
    "        :param data: a dataset\n",
    "        :param target: the list of class labels labelling data\n",
    "        :return:\n",
    "        For each target class:\n",
    "            1. yield prior_prob: the probability of each class. P(class) eg P(Iris-virginica)\n",
    "            2. yield summary: list of {'mean': 0.0,'stdev': 0.0} for every feature in data\n",
    "        \"\"\"\n",
    "        groups = self.group_by_class(data, target)\n",
    "        for index in range(groups.shape[0]):\n",
    "            group=groups[index]\n",
    "            self.summaries[self.target_values[index]] = {\n",
    "                'prior_prob': len(group)/len(data),\n",
    "                'summary': [i for i in self.summarize(group)]\n",
    "            }\n",
    "\n",
    "    def normal_pdf(self, x, mean, stdev):\n",
    "        \"\"\"\n",
    "        :param x: the value of a feature F\n",
    "        :param mean: Âµ - average of F\n",
    "        :param stdev: Ïƒ - standard deviation of F\n",
    "        :return: Gaussian (Normal) Density function.\n",
    "        N(x; Âµ, Ïƒ) = (1 / 2Ï€Ïƒ) * (e ^ (xâ€“Âµ)^2/-2Ïƒ^2\n",
    "        \"\"\"\n",
    "        variance = stdev ** 2\n",
    "        exp_squared_diff = (x - mean) ** 2\n",
    "        exp_power = -exp_squared_diff / (2 * variance)\n",
    "        exponent = e ** exp_power\n",
    "        denominator = ((2 * pi) ** .5) * stdev\n",
    "        normal_prob = exponent / denominator\n",
    "        return normal_prob\n",
    "\n",
    "    def marginal_pdf(self, joint_probabilities):\n",
    "        \"\"\"\n",
    "        :param joint_probabilities: list of joint probabilities for each feature\n",
    "        :return:\n",
    "        Marginal Probability Density Function (Predictor Prior Probability)\n",
    "        Joint Probability = prior * likelihood\n",
    "        Marginal Probability is the sum of all joint probabilities for all classes.\n",
    "        marginal_pdf =\n",
    "          [P(setosa) * P(sepal length | setosa) * P(sepal width | setosa) * P(petal length | setosa) * P(petal width | setosa)]\n",
    "        + [P(versicolour) * P(sepal length | versicolour) * P(sepal width | versicolour) * P(petal length | versicolour) * P(petal width | versicolour)]\n",
    "        + [P(virginica) * P(sepal length | verginica) * P(sepal width | verginica) * P(petal length | verginica) * P(petal width | verginica)]\n",
    "        \"\"\"\n",
    "        marginal_prob = sum(joint_probabilities.values())\n",
    "        return marginal_prob\n",
    "\n",
    "    def joint_probabilities(self, data):\n",
    "        \"\"\"\n",
    "        :param data: dataset in a matrix form (rows x col)\n",
    "        :return:\n",
    "        Use the normal_pdf(self, x, mean, stdev) to calculate the Normal Probability for each feature\n",
    "        Yields the product of all Normal Probabilities and the Prior Probability of the class.\n",
    "        \"\"\"\n",
    "        joint_probs = {}\n",
    "        for y in range(self.target_values.shape[0]):\n",
    "            target_v=self.target_values[y]\n",
    "            item=self.summaries[target_v]\n",
    "            total_features = len(item['summary'])\n",
    "            likelihood = 1\n",
    "            for index in range(total_features):\n",
    "                feature = data[index]\n",
    "                mean = self.summaries[target_v]['summary'][index]['mean']\n",
    "                stdev = self.summaries[target_v]['summary'][index]['stdev']**2\n",
    "                normal_prob = self.normal_pdf(feature,mean,stdev)\n",
    "                likelihood *= normal_prob\n",
    "            prior_prob = self.summaries[target_v]['prior_prob']\n",
    "            joint_probs[target_v] = prior_prob * likelihood\n",
    "        return joint_probs\n",
    "\n",
    "    def posterior_probabilities(self, test_row):\n",
    "        \"\"\"\n",
    "        :param test_row: single list of features to test; new data\n",
    "        :return:\n",
    "        For each feature (x) in the test_row:\n",
    "            1. Calculate Predictor Prior Probability using the Normal PDF N(x; Âµ, Ïƒ). eg = P(feature | class)\n",
    "            2. Calculate Likelihood by getting the product of the prior and the Normal PDFs\n",
    "            3. Multiply Likelihood by the prior to calculate the Joint Probability.\n",
    "        E.g.\n",
    "        prior_prob: P(setosa)\n",
    "        likelihood: P(sepal length | setosa) * P(sepal width | setosa) * P(petal length | setosa) * P(petal width | setosa)\n",
    "        joint_prob: prior_prob * likelihood\n",
    "        marginal_prob: predictor prior probability\n",
    "        posterior_prob = joint_prob/ marginal_prob\n",
    "        Yields a dictionary containing the posterior probability of every class\n",
    "        \"\"\"\n",
    "        posterior_probs = {}\n",
    "        joint_probabilities = self.joint_probabilities(test_row)\n",
    "        marginal_prob = self.marginal_pdf(joint_probabilities)\n",
    "        for y in range(self.target_values.shape[0]):\n",
    "            target_v=self.target_values[y]\n",
    "            joint_prob=joint_probabilities[target_v]\n",
    "            posterior_probs[target_v] = joint_prob / marginal_prob\n",
    "        return posterior_probs\n",
    "\n",
    "    def get_map(self, test_row):\n",
    "        \"\"\"\n",
    "        :param test_row: single list of features to test; new data\n",
    "        :return:\n",
    "        Return the target class with the largest posterior probability\n",
    "        \"\"\"\n",
    "        posterior_probs = self.posterior_probabilities(test_row)\n",
    "        target = max(posterior_probs, key=posterior_probs.get)\n",
    "        return target\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        :param data: test_data\n",
    "        :return:\n",
    "        Predict the likeliest target for each row of data.\n",
    "        Return a list of predicted targets.\n",
    "        \"\"\"\n",
    "        predicted_targets = []\n",
    "        for row in data:\n",
    "            predicted = self.get_map(row)\n",
    "            predicted_targets.append(predicted)\n",
    "        return predicted_targets\n",
    "\n",
    "    def accuracy(self, ground_true, predicted):\n",
    "        \"\"\"\n",
    "        :param ground_true: list of ground true classes of test_data\n",
    "        :param predicted: list of predicted classes\n",
    "        :return:\n",
    "        Calculate the the average performance of the classifier.\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        for x, y in zip(ground_true, predicted):\n",
    "            if x==y:\n",
    "                correct += 1\n",
    "        return correct / ground_true.shape[0]\n",
    "\n",
    "def main():\n",
    "    nb = GaussNB()\n",
    "    iris = datasets.load_iris()\n",
    "    data=iris.data\n",
    "    target=iris.target\n",
    "    nb.target_values=np.unique(target)\n",
    "    X_train,X_test,y_train,y_test=train_test_split(data,target,test_size=0.3)\n",
    "    nb.train(X_train,y_train)\n",
    "    predicted = nb.predict(X_test)\n",
    "    accuracy = nb.accuracy(y_test, predicted)\n",
    "    print('Accuracy: %.3f' % accuracy)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
