Part 1: Linear Regression
1. When can you use linear regression?

    When we want to approximate a function from a dataset. 

2. How can you generalize linear regression models to account for more complex relationships among the data?

    We can generalize linear regression by a polynomial basis or gaussian basis (there is more, but these 
    are the ones we go through in the paper).    

3. What are the basis functions?

    Polynomial basis and Gaussian basis (there is more, but these 
    are the ones we go through in the paper).    

4. How many basis functions can you use in the same regression model?

    There is no limit. But using more than necessary can lead to overfitting. 

5. Can overfitting be a problem? And if so, what can you do about it?

    Yes, overfitting is a problem because the model is trying to fit the training data too closely, thus 
    leading to a bad generalization of the data. 

    Solutions: 

    * Create a simpler model. 

    * Regularization - apply Ridge Regression or Lasso Regression.
      What is does is adding penalty terms, discouraging the model from assigning too large coefficients to variables. 

Part 2: K-Nearest Neighbor (KNN)
1. Why choosing a good value for k is important in KNN?

    It is important!
    
    Choosing a small value for k kan lead to underfitting, where to model captures random fluctuation
    rather than the underlying patterns. 

    Choosing a large value for k kan lead to overfitting, where the model fails to capture important
    local patterns in the data.

2. How can you decide a good value for k?

    There are multible ways of doing it:

    * Choose an odd value of k for 2 class problem as it will not be able to spit in two equals 
    * k must not be multiple of number of classes. 
    * Trail and error: test different k and evaluate the fitting.  
    * Elbow Method: Plot error rate agains different k:s in a test set. Than choose the k value that is located 
      in the "elbow". 

3. Can you use KNN to classify non-linearly separable data?

    Yes (according to the lab2 instructions - search nonlinear to find it).

4. Is KNN sensible to the number of features in the dataset?

    Yes, if there are a lot of features and possible noise in the labels, the decision will become overfitting
    (look at lecture 3) 

5. Can you use KNN for a regression problem?

    Yes. 

6. What are the Pros and Cons of KNN?

    Pros: 
    * No need for training 
    * Can be used to nonlinear problems and regression problems 
    * Fast 

    Cons: 
    * Testing phase is slow and costlier, need a lot of memory to for storing training dataset
    * Euclidean distance is sensitive to magnitudes, high magnitudes weight more 


Part 3: Support Vector Machines (SVM)
1. What is the basic idea/intuition of SVM?

    The main idea of SVM is to find hyperplanes that separates data points into classes.The ideal 
    case is to maximize the margins (giving better seperation betwen the classes).

2. What can you do if the dataset is not linearly separable?

    Use the Kernal trick which gives a non linear boundary separating the data points

3. Explain the concept of Soften Margins

    We can use the C parameter (good for overlap). 
    
    C -> large: small margins 
    C -> small: large margins 

4. What are the pros and cons of SVM?

    Pros:
    * Take very little memory.
    * Once the model is trained, the prediction phase is very fast.
    * Works well with high dimensionall data (because they are affected only by points near the margin).
    * If a clear margin is given, SVM works realy well

    Cons:
    * Big data sets gives high training time
    * Dont handel noise well
    * Have no probobility estimation
